{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "702c03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a972ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from flashrank import Ranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import FlashrankRerank\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b1be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 pages.\n",
      "First page preview:\n",
      " Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aid\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"data/attention-is-all-you-need-Paper.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} pages.\")\n",
    "print(\"First page preview:\\n\", docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d567e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 chunks created\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aid\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"{len(chunks)} chunks created\")\n",
    "print(chunks[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "668ea09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viswa\\AppData\\Local\\Temp\\ipykernel_11048\\1419910090.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs_retrieved = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 8 chunks:\n",
      "\n",
      "--- Document 1 Content ---\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@goo\n",
      "\n",
      "--- Document 2 Content ---\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [14, 15] and [8].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction mode\n",
      "\n",
      "--- Document 3 Content ---\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 16]. In all but a fe\n",
      "\n",
      "--- Document 4 Content ---\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      "BLEU, establishing a\n",
      "\n",
      "--- Document 5 Content ---\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNor\n",
      "\n",
      "--- Document 6 Content ---\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks,\n",
      "\n",
      "--- Document 7 Content ---\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "Model\n",
      "BLEU Training Cost (FLOPs)\n",
      "EN-DE EN-FR EN-DE EN-FR\n",
      "\n",
      "\n",
      "--- Document 8 Content ---\n",
      "transduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [31, 21, 13].\n",
      "∗Equal contribution. Listin\n"
     ]
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "query = \"What is transformer?\"\n",
    "docs_retrieved = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"Retrieved {len(docs_retrieved)} chunks:\")\n",
    "for i, d in enumerate(docs_retrieved, 1):\n",
    "    print(f\"\\n--- Document {i} Content ---\\n{d.page_content[:250]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e9b07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reranked to top 4 docs:\n",
      "\n",
      "--- Reranked Document 1 (score: 0.7896642684936523) ---\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNor\n",
      "\n",
      "--- Reranked Document 2 (score: 0.7049769163131714) ---\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 16]. In all but a fe\n",
      "\n",
      "--- Reranked Document 3 (score: 0.5463269948959351) ---\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [14, 15] and [8].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction mode\n",
      "\n",
      "--- Reranked Document 4 (score: 0.4705306887626648) ---\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks,\n"
     ]
    }
   ],
   "source": [
    "reranker = FlashrankRerank(model=\"ms-marco-MiniLM-L-12-v2\", top_n=4)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=retriever,\n",
    "    base_compressor=reranker\n",
    ")\n",
    "\n",
    "docs_reranked = compression_retriever.invoke(query)\n",
    "\n",
    "print(f\"\\nReranked to top {len(docs_reranked)} docs:\")\n",
    "for i, d in enumerate(docs_reranked, 1):\n",
    "    meta_score = d.metadata.get(\"relevance_score\", None)\n",
    "    print(f\"\\n--- Reranked Document {i} (score: {meta_score}) ---\\n{d.page_content[:250]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d438c84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer:\n",
      " The Transformer is a model architecture designed for sequence transduction tasks, which relies entirely on an attention mechanism to model dependencies between input and output sequences, rather than using recurrent or convolutional networks. It consists of an encoder-decoder structure, where the encoder maps an input sequence to continuous representations, and the decoder generates an output sequence one element at a time in an auto-regressive manner. The Transformer architecture allows for significant parallelization, enabling faster training compared to traditional models, and has achieved state-of-the-art results in translation tasks. It employs multi-headed self-attention and point-wise fully connected layers, with residual connections and layer normalization to enhance performance. The model is particularly noted for its efficiency and effectiveness in handling various sequence modeling tasks.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "context = \"\\n\\n\".join([d.page_content for d in docs_reranked])\n",
    "prompt = (\n",
    "    f\"Using the following context, answer the question:\\n\\nContext:\\n{context}\"\n",
    "    f\"\\n\\nQuestion:\\n{query}\"\n",
    ")\n",
    "response = llm.invoke(prompt)\n",
    "print(\"\\nGenerated Answer:\\n\", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "143bd0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer from LangGraph Pipeline:\n",
      " The Transformer is a model architecture designed for sequence transduction tasks, which relies entirely on an attention mechanism to model dependencies between input and output sequences, rather than using recurrent or convolutional networks. It consists of an encoder-decoder structure, where the encoder maps an input sequence to continuous representations, and the decoder generates an output sequence one element at a time in an auto-regressive manner. The Transformer architecture allows for significant parallelization, enabling faster training compared to traditional models, and has achieved state-of-the-art results in translation tasks. It employs stacked self-attention and fully connected layers, with mechanisms like residual connections and layer normalization to enhance performance. The Transformer is also adaptable for various tasks beyond text, including images, audio, and video.\n"
     ]
    }
   ],
   "source": [
    "def retrieve_stage(state):\n",
    "    docs_ = compression_retriever.get_relevant_documents(state[\"question\"])\n",
    "    state[\"context\"] = \"\\n\\n\".join(d.page_content for d in docs_)\n",
    "    return state\n",
    "\n",
    "def generate_stage(state):\n",
    "    prompt = (\n",
    "        f\"Using the context below, answer the question:\\n\\nContext:\\n{state['context']}\"\n",
    "        f\"\\n\\nQuestion:\\n{state['question']}\"\n",
    "    )\n",
    "    state[\"answer\"] = llm.invoke(prompt).content\n",
    "    return state\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"retrieve\", retrieve_stage)\n",
    "graph.add_node(\"generate\", generate_stage)\n",
    "graph.add_edge(START, \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", END)\n",
    "\n",
    "rag_pipeline = graph.compile()\n",
    "\n",
    "state = {\"question\": query}\n",
    "result = rag_pipeline.invoke(state)\n",
    "print(\"\\nFinal Answer from LangGraph Pipeline:\\n\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdccc554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
