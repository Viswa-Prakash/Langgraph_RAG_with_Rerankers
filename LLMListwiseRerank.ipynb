{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9fda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3d9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors.listwise_rerank import LLMListwiseRerank\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf5e7a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 pages\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aid\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"data/attention-is-all-you-need-Paper.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} pages\")\n",
    "print(docs[0].page_content[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982bfda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 51\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aid\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(chunks[0].page_content[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b54057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viswa\\AppData\\Local\\Temp\\ipykernel_13896\\3726667733.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs_retrieved = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 8 docs\n",
      "\n",
      "--- Doc 1 ---\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in qua\n",
      "\n",
      "--- Doc 2 ---\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@goo\n",
      "\n",
      "--- Doc 3 ---\n",
      "results to the base model.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed\n",
      "\n",
      "--- Doc 4 ---\n",
      "textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-l\n",
      "\n",
      "--- Doc 5 ---\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\n",
      "of continuous representations z = (z\n",
      "\n",
      "--- Doc 6 ---\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[20], ByteNet [15] and ConvS2S [8], all of which u\n",
      "\n",
      "--- Doc 7 ---\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNor\n",
      "\n",
      "--- Doc 8 ---\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance i\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()  # Uses OPENAI_API_KEY from .env\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "query = \"What is transformer?\"\n",
    "docs_retrieved = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"Retrieved {len(docs_retrieved)} docs\")\n",
    "for i, d in enumerate(docs_retrieved, 1):\n",
    "    print(f\"\\n--- Doc {i} ---\\n{d.page_content[:250]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c929b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reranked to top 4 docs:\n",
      "\n",
      "--- Reranked Doc 1 ---\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@goo\n",
      "\n",
      "--- Reranked Doc 2 ---\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in qua\n",
      "\n",
      "--- Reranked Doc 3 ---\n",
      "results to the base model.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed\n",
      "\n",
      "--- Reranked Doc 4 ---\n",
      "textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-l\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "reranker = LLMListwiseRerank.from_llm(llm=llm, top_n=4)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=retriever,\n",
    "    base_compressor=reranker\n",
    ")\n",
    "\n",
    "docs_reranked = compression_retriever.invoke(query)\n",
    "\n",
    "print(f\"\\nReranked to top {len(docs_reranked)} docs:\")\n",
    "for i, d in enumerate(docs_reranked, 1):\n",
    "    print(f\"\\n--- Reranked Doc {i} ---\\n{d.page_content[:250]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d15ac4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer:\n",
      " The Transformer is a novel network architecture proposed for sequence transduction tasks, which relies entirely on attention mechanisms rather than using recurrent or convolutional layers. It is designed to process input and output sequences by employing multi-headed self-attention to compute representations, allowing for significantly faster training compared to traditional models that utilize recurrent or convolutional structures. The Transformer has demonstrated superior performance in machine translation tasks, achieving state-of-the-art results on benchmarks such as the WMT 2014 English-to-German and English-to-French translation tasks.\n"
     ]
    }
   ],
   "source": [
    "context = \"\\n\\n\".join([d.page_content for d in docs_reranked])\n",
    "\n",
    "prompt = f\"Using the context below, answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{query}\"\n",
    "\n",
    "answer = llm.invoke(prompt)\n",
    "\n",
    "print(\"\\nGenerated Answer:\\n\", answer.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f27c1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer from LangGraph Pipeline:\n",
      " The Transformer is a novel network architecture proposed for sequence transduction tasks, which relies entirely on attention mechanisms rather than traditional recurrent or convolutional layers. It is designed to process input and output sequences by using multi-headed self-attention to compute representations, allowing for significantly faster training compared to previous models. The Transformer has demonstrated superior performance in machine translation tasks, achieving state-of-the-art results on benchmarks such as the WMT 2014 English-to-German and English-to-French translation tasks. Its architecture enables better parallelization and reduces training time, making it a significant advancement in the field of natural language processing.\n"
     ]
    }
   ],
   "source": [
    "def retrieve_stage(state):\n",
    "    docs_ = compression_retriever.get_relevant_documents(state[\"question\"])\n",
    "    state[\"context\"] = \"\\n\\n\".join(d.page_content for d in docs_)\n",
    "    return state\n",
    "\n",
    "def generate_stage(state):\n",
    "    prompt = f\"Using the context below, answer the question:\\n\\nContext:\\n{state['context']}\\n\\nQuestion:\\n{state['question']}\"\n",
    "    state[\"answer\"] = llm.invoke(prompt).content\n",
    "    return state\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"retrieve\", retrieve_stage)\n",
    "graph.add_node(\"generate\", generate_stage)\n",
    "graph.add_edge(START, \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", END)\n",
    "\n",
    "rag_pipeline = graph.compile()\n",
    "\n",
    "state = {\"question\": query}\n",
    "result = rag_pipeline.invoke(state)\n",
    "print(\"\\nFinal Answer from LangGraph Pipeline:\\n\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe9e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
