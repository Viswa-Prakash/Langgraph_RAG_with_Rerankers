{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cbc28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"CONTEXTUAL_AI_API_KEY\"] = os.getenv(\"CONTEXTUAL_AI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be12f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_contextual import ContextualRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8975376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 pages\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aid\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"data/attention-is-all-you-need-Paper.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} pages\")\n",
    "print(docs[0].page_content[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6ba73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 51\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aid\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(chunks[0].page_content[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5accc610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viswa\\AppData\\Local\\Temp\\ipykernel_10032\\4072380253.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs_retrieved = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 8 docs\n",
      "\n",
      "--- Doc 1 ---\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in qua\n",
      "\n",
      "--- Doc 2 ---\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@goo\n",
      "\n",
      "--- Doc 3 ---\n",
      "results to the base model.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed\n",
      "\n",
      "--- Doc 4 ---\n",
      "textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-l\n",
      "\n",
      "--- Doc 5 ---\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\n",
      "of continuous representations z = (z\n",
      "\n",
      "--- Doc 6 ---\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[20], ByteNet [15] and ConvS2S [8], all of which u\n",
      "\n",
      "--- Doc 7 ---\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNor\n",
      "\n",
      "--- Doc 8 ---\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance i\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "query = \"What is transformer?\"\n",
    "docs_retrieved = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"Retrieved {len(docs_retrieved)} docs\")\n",
    "for i, d in enumerate(docs_retrieved, 1):\n",
    "    print(f\"\\n--- Doc {i} ---\\n{d.page_content[:250]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e5802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reranked to top 8 docs:\n",
      "\n",
      "--- Reranked Doc 1 ---\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in qua\n",
      "\n",
      "--- Reranked Doc 2 ---\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@goo\n",
      "\n",
      "--- Reranked Doc 3 ---\n",
      "results to the base model.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed\n",
      "\n",
      "--- Reranked Doc 4 ---\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\n",
      "of continuous representations z = (z\n",
      "\n",
      "--- Reranked Doc 5 ---\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance i\n",
      "\n",
      "--- Reranked Doc 6 ---\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNor\n",
      "\n",
      "--- Reranked Doc 7 ---\n",
      "textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-l\n",
      "\n",
      "--- Reranked Doc 8 ---\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[20], ByteNet [15] and ConvS2S [8], all of which u\n"
     ]
    }
   ],
   "source": [
    "reranker = ContextualRerank(model=\"ctxl-rerank-en-v1-instruct\", top_n=4)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=retriever,\n",
    "    base_compressor=reranker\n",
    ")\n",
    "\n",
    "docs_reranked = compression_retriever.invoke(query)\n",
    "\n",
    "print(f\"\\nReranked to top {len(docs_reranked)} docs:\")\n",
    "for i, d in enumerate(docs_reranked, 1):\n",
    "    print(f\"\\n--- Reranked Doc {i} ---\\n{d.page_content[:250]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer:\n",
      " The Transformer is a novel network architecture designed for sequence transduction tasks, such as machine translation, that relies entirely on attention mechanisms rather than recurrent or convolutional layers. It consists of an encoder-decoder structure where the encoder processes the input sequence and the decoder generates the output sequence. The key innovation of the Transformer is its use of multi-headed self-attention, which allows it to model dependencies between input and output elements without regard to their distance in the sequence. This architecture enables significantly more parallelization during training, resulting in faster training times and improved translation quality. The Transformer has achieved state-of-the-art results in various translation tasks, outperforming previous models that relied on recurrent or convolutional architectures.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "context = \"\\n\\n\".join([d.page_content for d in docs_reranked])\n",
    "\n",
    "prompt = f\"Using the context below, answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{query}\"\n",
    "\n",
    "answer = llm.invoke(prompt)\n",
    "\n",
    "print(\"\\nGenerated Answer:\\n\", answer.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a26c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer from LangGraph Pipeline:\n",
      " The Transformer is a novel network architecture designed for sequence transduction tasks, such as machine translation, that relies entirely on attention mechanisms rather than recurrent or convolutional layers. It consists of an encoder-decoder structure where the encoder processes an input sequence and the decoder generates an output sequence. The key innovation of the Transformer is its use of multi-headed self-attention, which allows it to model dependencies between input and output elements without regard to their distance in the sequence. This architecture enables significantly more parallelization during training, resulting in faster training times and improved translation quality. The Transformer has achieved state-of-the-art results in various translation tasks, outperforming previous models that relied on recurrent or convolutional architectures.\n"
     ]
    }
   ],
   "source": [
    "def retrieve_stage(state):\n",
    "    docs_ = compression_retriever.get_relevant_documents(state[\"question\"])\n",
    "    state[\"context\"] = \"\\n\\n\".join(d.page_content for d in docs_)\n",
    "    return state\n",
    "\n",
    "def generate_stage(state):\n",
    "    prompt = f\"Using the context below, answer the question:\\n\\nContext:\\n{state['context']}\\n\\nQuestion:\\n{state['question']}\"\n",
    "    state[\"answer\"] = llm.invoke(prompt).content\n",
    "    return state\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"retrieve\", retrieve_stage)\n",
    "graph.add_node(\"generate\", generate_stage)\n",
    "graph.add_edge(START, \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", END)\n",
    "\n",
    "rag_pipeline = graph.compile()\n",
    "\n",
    "state = {\"question\": query}\n",
    "result = rag_pipeline.invoke(state)\n",
    "print(\"\\nFinal Answer from LangGraph Pipeline:\\n\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b674f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
