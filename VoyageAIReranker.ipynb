{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9a372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"VOYAGE_API_KEY\"] = os.getenv(\"VOYAGE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffde043e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\viswa\\Documents\\GENAIandAGENTICAI\\Langgraph_RAG_with_Rerankers\\reranker\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_voyageai import VoyageAIRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a62b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages loaded: 11\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. nis the sequence length, dis the representation dimension, kis the kernel\n",
      "size of convolutions and rthe size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2 ·d) O(1) O(1)\n",
      "Recurrent O(n·d2) O(n) O(n)\n",
      "Convolutional O(k·n·d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r·n·d) O(1) \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and split PDF\n",
    "loader = PyPDFLoader(\"data/attention-is-all-you-need-Paper.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Check\n",
    "print(f\"Total pages loaded: {len(docs)}\")\n",
    "print(docs[5].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca51f8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 43\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Check\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(chunks[5].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cbfd121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create FAISS vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5357d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Base retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec07ac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viswa\\AppData\\Local\\Temp\\ipykernel_21528\\1159692597.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 documents\n",
      "Rank Doc 1:\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz\n",
      "Rank Doc 2:\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [14, 15] and [8].\n",
      "3 Model Architecture\n",
      "\n",
      "Rank Doc 3:\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the \n",
      "Rank Doc 4:\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (includin\n",
      "Rank Doc 5:\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1].\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "query = \"What is transformer?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(f\"Rank Doc {i}:\\n{doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc5cb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked to 3 documents\n",
      "Rank Doc 1:\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the \n",
      "Rank Doc 2:\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (includin\n",
      "Rank Doc 3:\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Add VoyageAI reranker\n",
    "compressor = VoyageAIRerank(\n",
    "    model=\"rerank-lite-1\",             # or 'rerank-1', 'rerank-2', etc.\n",
    "    top_k=3                            # number of top docs to keep\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "reranked_docs = compression_retriever.invoke(query)\n",
    "print(f\"Reranked to {len(reranked_docs)} documents\")\n",
    "\n",
    "for i, doc in enumerate(reranked_docs, start=1):\n",
    "    print(f\"Rank Doc {i}:\\n{doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe237a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The Transformer is a model architecture designed for sequence modeling and transduction tasks that relies entirely on attention mechanisms, eliminating the need for recurrence and convolutions. This architecture allows for the modeling of dependencies between input and output sequences without regard to their distance, enabling significant parallelization during training. The Transformer has demonstrated superior performance in machine translation tasks, achieving state-of-the-art results while requiring less training time and cost compared to previous models that utilized recurrent or convolutional neural networks.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: LLM\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "context = \"\\n\\n\".join([d.page_content for d in reranked_docs])\n",
    "prompt = f\"Answer the following question based on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "answer = llm.invoke(prompt)\n",
    "print(\"Answer:\", answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8351aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Answer: The Transformer is a model architecture that relies entirely on attention mechanisms to model dependencies between input and output sequences, without using recurrence or convolutions. This design allows for significantly more parallelization in processing, leading to improved efficiency and performance in tasks such as machine translation. The Transformer has been shown to achieve state-of-the-art results in translation quality, outperforming previous models while requiring less training time and cost.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: LangGraph pipeline\n",
    "def retrieve_docs(state):\n",
    "    docs = compression_retriever.get_relevant_documents(state[\"question\"])\n",
    "    state[\"context\"] = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    return state\n",
    "\n",
    "def generate_answer(state):\n",
    "    prompt = f\"Answer the question using the context:\\n\\nContext:\\n{state['context']}\\n\\nQuestion:\\n{state['question']}\"\n",
    "    state[\"answer\"] = llm.invoke(prompt).content\n",
    "    return state\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"retrieve\", retrieve_docs)\n",
    "graph.add_node(\"generate\", generate_answer)\n",
    "graph.add_edge(START, \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", END)\n",
    "\n",
    "app_graph = graph.compile()\n",
    "\n",
    "# Test full pipeline\n",
    "state = {\"question\": query}\n",
    "result = app_graph.invoke(state)\n",
    "print(\"Pipeline Answer:\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133fcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
