{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e193dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"JINA_API_KEY\"] = os.getenv(\"JINA_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ecbdd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_community.document_compressors import JinaRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba606270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages loaded: 11\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. nis the sequence length, dis the representation dimension, kis the kernel\n",
      "size of convolutions and rthe size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2 ·d) O(1) O(1)\n",
      "Recurrent O(n·d2) O(n) O(n)\n",
      "Convolutional O(k·n·d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r·n·d) O(1) \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and split PDF\n",
    "loader = PyPDFLoader(\"data/attention-is-all-you-need-Paper.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Check\n",
    "print(f\"Total pages loaded: {len(docs)}\")\n",
    "print(docs[5].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f394af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 43\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Check\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(chunks[5].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f91f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create FAISS vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b75c2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Base retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d876f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viswa\\AppData\\Local\\Temp\\ipykernel_14480\\1159692597.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 documents\n",
      "Rank Doc 1:\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz\n",
      "Rank Doc 2:\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [14, 15] and [8].\n",
      "3 Model Architecture\n",
      "\n",
      "Rank Doc 3:\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the \n",
      "Rank Doc 4:\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (includin\n",
      "Rank Doc 5:\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1].\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "query = \"What is transformer?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(f\"Rank Doc {i}:\\n{doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90fa1ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked to 3 documents\n",
      "Rank Doc 1:\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [14, 15] and [8].\n",
      "3 Model Architecture\n",
      "\n",
      "Rank Doc 2:\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the \n",
      "Rank Doc 3:\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1].\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Add Jina reranker\n",
    "compressor = JinaRerank(\n",
    "    model=\"jina-reranker-v2-base-multilingual\"\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "reranked_docs = compression_retriever.invoke(query)\n",
    "print(f\"Reranked to {len(reranked_docs)} documents\")\n",
    "\n",
    "for i, doc in enumerate(reranked_docs, start=1):\n",
    "    print(f\"Rank Doc {i}:\\n{doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf72d34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The Transformer is a model architecture designed for sequence transduction tasks, which utilizes an encoder-decoder structure. Unlike traditional models that rely on recurrent neural networks (RNNs), the Transformer employs stacked self-attention mechanisms and fully connected layers to process input and output sequences. The encoder maps an input sequence of symbols to continuous representations, while the decoder generates an output sequence one element at a time, using previously generated symbols as additional input.\n",
      "\n",
      "The key features of the Transformer include:\n",
      "\n",
      "1. **Attention Mechanism**: It allows the model to capture dependencies between input and output sequences without regard to their distance, enabling more effective modeling of relationships in the data.\n",
      "\n",
      "2. **Parallelization**: By eschewing recurrence, the Transformer allows for significant parallel processing, which can lead to faster training times and improved performance.\n",
      "\n",
      "3. **Architecture**: The model consists of multiple identical layers in both the encoder and decoder, with residual connections and layer normalization applied to facilitate training.\n",
      "\n",
      "Overall, the Transformer has achieved state-of-the-art results in various tasks, particularly in translation, and can be trained efficiently on modern hardware.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: LLM\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "context = \"\\n\\n\".join([d.page_content for d in reranked_docs])\n",
    "prompt = f\"Answer the following question based on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "answer = llm.invoke(prompt)\n",
    "print(\"Answer:\", answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab6274fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Answer: The Transformer is a model architecture designed for sequence transduction tasks, which utilizes an encoder-decoder structure. Unlike traditional models that rely on recurrent networks, the Transformer employs stacked self-attention mechanisms and point-wise, fully connected layers to map input sequences to continuous representations and generate output sequences. This architecture allows for significant parallelization, enabling faster training and improved translation quality. The encoder processes the input sequence to produce continuous representations, while the decoder generates the output sequence one element at a time, using previously generated symbols as additional input. The Transformer is notable for its ability to model dependencies without regard to their distance in the input or output sequences, making it a powerful tool in various sequence modeling tasks.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: LangGraph pipeline\n",
    "def retrieve_docs(state):\n",
    "    docs = compression_retriever.get_relevant_documents(state[\"question\"])\n",
    "    state[\"context\"] = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    return state\n",
    "\n",
    "def generate_answer(state):\n",
    "    prompt = f\"Answer the question using the context:\\n\\nContext:\\n{state['context']}\\n\\nQuestion:\\n{state['question']}\"\n",
    "    state[\"answer\"] = llm.invoke(prompt).content\n",
    "    return state\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"retrieve\", retrieve_docs)\n",
    "graph.add_node(\"generate\", generate_answer)\n",
    "graph.add_edge(START, \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", END)\n",
    "\n",
    "app_graph = graph.compile()\n",
    "\n",
    "# Test full pipeline\n",
    "state = {\"question\": query}\n",
    "result = app_graph.invoke(state)\n",
    "print(\"Pipeline Answer:\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5089461b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
