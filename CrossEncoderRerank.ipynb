{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c559d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c3d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c025b649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 2\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brai\n"
     ]
    }
   ],
   "source": [
    "# Load and preview the PDF\n",
    "loader = PyPDFLoader(\"data/attention-is-all-you-need-Paper.pdf\")\n",
    "all_docs = loader.load()\n",
    "\n",
    "# Process only the first few pages (e.g., first 2 pages)\n",
    "num_pages_to_process = 2\n",
    "docs = all_docs[:num_pages_to_process]\n",
    "\n",
    "print(f\"Loaded: {len(docs)}\")\n",
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58821dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into: 10\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brai\n"
     ]
    }
   ],
   "source": [
    "# Split into managable chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split into: {len(chunks)}\")\n",
    "print(chunks[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7360434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings and vector store\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2685e422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 8 documents\n",
      "Rank Doc 1:\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz\n",
      "Rank Doc 2:\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [14, 15] and [8].\n",
      "3 Model Architecture\n",
      "\n",
      "Rank Doc 3:\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the \n",
      "Rank Doc 4:\n",
      "transduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "archi\n",
      "Rank Doc 5:\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the seque\n",
      "Rank Doc 6:\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\n",
      "bl\n",
      "Rank Doc 7:\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
      "less time to train. Our model achieves 28.\n",
      "Rank Doc 8:\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "stat\n"
     ]
    }
   ],
   "source": [
    "# Rank retrieval test\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "query = \"What is transformer?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(f\"Rank Doc {i}:\\n{doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55716d43",
   "metadata": {},
   "source": [
    "**Requires python 3.10 to work properly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85492806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\viswa\\Documents\\GENAIandAGENTICAI\\Langgraph_RAG_with_Rerankers\\reranker\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\viswa\\Documents\\GENAIandAGENTICAI\\Langgraph_RAG_with_Rerankers\\reranker\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\viswa\\.cache\\huggingface\\hub\\models--BAAI--bge-reranker-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Reranked 3 documents\n",
      "Rank Doc 1:\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the seque\n",
      "Rank Doc 2:\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the \n",
      "Rank Doc 3:\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\n",
      "bl\n"
     ]
    }
   ],
   "source": [
    "# Load a cross-encoder model\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "\n",
    "# Create the CrossEncoderReranker\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "\n",
    "# Combine the base retriever with the reranker\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "reranked = compression_retriever.invoke(query)\n",
    "print(f\"Retrieved Reranked {len(reranked)} documents\")\n",
    "\n",
    "for i, doc in enumerate(reranked, start=1):\n",
    "    print(f\"Rank Doc {i}:\\n{doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "857e34fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer:\n",
      " The Transformer is a model architecture that relies entirely on self-attention mechanisms to compute representations of its input and output, without using recurrent neural networks (RNNs) or convolutional layers. This design allows the Transformer to model dependencies between input and output sequences without regard to their distance, enabling significant parallelization during training. As a result, the Transformer can achieve state-of-the-art performance in tasks such as translation, often requiring less training time compared to traditional models that use recurrence. The architecture is particularly effective in handling long-range dependencies in sequences, which is a challenge for models that rely on sequential computation.\n"
     ]
    }
   ],
   "source": [
    "# Generate answer using top reranked context\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "context = \"\\n\\n\".join([d.page_content for d in reranked])\n",
    "prompt = (f\"Use the context below to answer the question.\\n\\nContext:\\n{context}\"\n",
    "          f\"\\n\\nQuestion:\\n{query}\")\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(\"\\nGenerated Answer:\\n\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c98c63b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viswa\\AppData\\Local\\Temp\\ipykernel_8028\\700103416.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = compression_retriever.get_relevant_documents(state[\"question\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer from LangGraph Pipeline:\n",
      " The Transformer is a model architecture that relies entirely on self-attention mechanisms to compute representations of its input and output, without using recurrent neural networks (RNNs) or convolutional layers. This design allows the Transformer to model dependencies between input and output sequences without regard to their distance, enabling significant parallelization during training. As a result, the Transformer can achieve state-of-the-art performance in tasks such as translation, often requiring less training time compared to traditional models that use recurrence. The architecture is particularly effective in handling long-range dependencies in sequences, as it reduces the number of operations needed to relate signals from different positions to a constant number, which contrasts with other models where the number of operations grows with the distance between positions.\n"
     ]
    }
   ],
   "source": [
    "# Wrap with LangGraph and test end-to-end\n",
    "def retrieve_stage(state):\n",
    "    docs = compression_retriever.get_relevant_documents(state[\"question\"])\n",
    "    state[\"context\"] = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    return state\n",
    "\n",
    "def generate_stage(state):\n",
    "    prompt = (f\"Use the context below to answer the question.\\n\\nContext:\\n{state['context']}\"\n",
    "              f\"\\n\\nQuestion:\\n{state['question']}\")\n",
    "    state[\"answer\"] = llm.invoke(prompt).content\n",
    "    return state\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"retrieve\", retrieve_stage)\n",
    "graph.add_node(\"generate\", generate_stage)\n",
    "graph.add_edge(START, \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", END)\n",
    "\n",
    "rag_pipeline = graph.compile()\n",
    "\n",
    "state = {\"question\": query}\n",
    "result = rag_pipeline.invoke(state)\n",
    "print(\"\\nFinal Answer from LangGraph Pipeline:\\n\", result[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
